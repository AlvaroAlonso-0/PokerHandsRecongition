{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Card classification using VGG16</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary libraries and modules for building the deep learning model that uses transfer learning with the VGG16 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a new Keras model using the pre-trained VGG16 architecture, with the ImageNet weights. It then adds some new layers on top of the VGG16 model's output, freezes the weights of the VGG16 layers, and compiles the new model with the Adam optimizer and categorical cross-entropy loss. The model is designed for a classification task with 53 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 75, 64)       0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 75, 128)      73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 75, 128)      147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 9, 512)        0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 9, 512)        2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 9, 512)        2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 9, 512)        2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 4, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14336)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                917568    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 53)                3445      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,635,701\n",
      "Trainable params: 921,013\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Charge model VGC16 with imagenet weights\n",
    "model_vgc = VGG16(weights='imagenet', include_top=False, input_shape=(224, 150, 3))\n",
    "for(layer) in model_vgc.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = model_vgc.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(53, activation='softmax')(x)  \n",
    "\n",
    "model = Model(inputs=model_vgc.input, outputs=x)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up a data generator using Keras *ImageDataGenerator* class. It sets the data directory and specifies parameters for data augmentation and preprocessing, such as flipping images horizontally and using the *preprocess_input* function to preprocess the input images. It then uses the *flow_from_directory* method of the *ImageDataGenerator* class to generate batches of data from a directory of images for both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4248 images belonging to 53 classes.\n",
      "Found 1060 images belonging to 53 classes.\n"
     ]
    }
   ],
   "source": [
    "# Folder with input data\n",
    "folder = 'images/training/labeled/'\n",
    "\n",
    "# Data generator\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True, validation_split=0.2)\n",
    "train_generator = datagen.flow_from_directory(folder, target_size=(224, 150), batch_size=53, class_mode='categorical', subset='training')\n",
    "test_generator = datagen.flow_from_directory(folder, target_size=(224, 150), batch_size=53, class_mode='categorical', subset='validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block trains the Keras model defined earlier on the data generated by *train_generator* and *test_generator* for 10 epochs and saves the trained model and training history. The trained model is saved in the file *model_fulldeck_v2.h5* and the training history is saved in the file *history_v2.npy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "81/81 [==============================] - 29s 236ms/step - loss: 3.1350 - accuracy: 0.2839 - val_loss: 1.5102 - val_accuracy: 0.5594\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 13s 158ms/step - loss: 0.7221 - accuracy: 0.7801 - val_loss: 0.3073 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 13s 159ms/step - loss: 0.1422 - accuracy: 0.9569 - val_loss: 0.1659 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 13s 156ms/step - loss: 0.0525 - accuracy: 0.9849 - val_loss: 0.0673 - val_accuracy: 0.9774\n",
      "Epoch 5/10\n",
      "81/81 [==============================] - 13s 156ms/step - loss: 0.0497 - accuracy: 0.9889 - val_loss: 0.0900 - val_accuracy: 0.9717\n",
      "Epoch 6/10\n",
      "81/81 [==============================] - 13s 158ms/step - loss: 0.0362 - accuracy: 0.9929 - val_loss: 0.1208 - val_accuracy: 0.9792\n",
      "Epoch 7/10\n",
      "81/81 [==============================] - 13s 156ms/step - loss: 0.0349 - accuracy: 0.9920 - val_loss: 0.1585 - val_accuracy: 0.9632\n",
      "Epoch 8/10\n",
      "81/81 [==============================] - 14s 171ms/step - loss: 0.0164 - accuracy: 0.9965 - val_loss: 0.0717 - val_accuracy: 0.9849\n",
      "Epoch 9/10\n",
      "81/81 [==============================] - 13s 162ms/step - loss: 0.0508 - accuracy: 0.9882 - val_loss: 0.0340 - val_accuracy: 0.9915\n",
      "Epoch 10/10\n",
      "81/81 [==============================] - 13s 157ms/step - loss: 0.0439 - accuracy: 0.9868 - val_loss: 0.1946 - val_accuracy: 0.9594\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "model.save('model_fulldeck_v2.h5')\n",
    "np.save('history_v2.npy',history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
